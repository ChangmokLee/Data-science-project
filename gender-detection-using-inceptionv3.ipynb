{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"0bbc0f3fd3cac3ba420b59ffab6248f589b2080c"},"source":["## Gender Detection using InceptionV3\n","\n","Image recognition is one of the many applications of Machine Learning, it can solve problems for security purposes, object detection, face detection, healthcare, entertainment, among others. This application has an enormous potential to help our society, so it is important to find new uses for this tool, improve the current methods and get more accurate and useful insights from it.\n","\n","In this project, we will build a Machine Learning Algorithm using CNN to predict from a giving picture if the celebrity is male or female.\n","\n","---\n","\n","## Dataset\n","\n","For this project we will use the CelebA dataset (http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html), which is available on Kaggle.\n","\n","Description of the CelebA dataset from kaggle (https://www.kaggle.com/jessicali9530/celeba-dataset): \n","\n","### Context\n","\n","A popular component of computer vision and deep learning revolves around identifying faces for various applications from logging into your phone with your face or searching through surveillance images for a particular suspect. This dataset is great for training and testing models for face detection, particularly for recognising facial attributes such as finding people with brown hair, are smiling, or wearing glasses. Images cover large pose variations, background clutter, diverse people, supported by a large quantity of images and rich annotations. This data was originally collected by researchers at MMLAB, The Chinese University of Hong Kong (specific reference in Acknowledgment section).\n","\n","### Content\n","\n","#### Overall\n","\n","202,599 number of face images of various celebrities\n","10,177 unique identities, but names of identities are not given\n","40 binary attribute annotations per image\n","5 landmark locations\n","\n","#### Data Files\n","\n","- <b>img_align_celeba.zip</b>: All the face images, cropped and aligned\n","- <b>list_eval_partition.csv</b>: Recommended partitioning of images into training, validation, testing sets. Images 1-162770 are training, 162771-182637 are validation, 182638-202599 are testing\n","- <b>list_bbox_celeba.csv</b>: Bounding box information for each image. \"x_1\" and \"y_1\" represent the upper left point coordinate of bounding box. \"width\" and \"height\" represent the width and height of bounding box\n","- <b>list_landmarks_align_celeba.csv</b>: Image landmarks and their respective coordinates. There are 5 landmarks: left eye, right eye, nose, left mouth, right mouth\n","- <b>list_attr_celeba.csv</b>: Attribute labels for each image. There are 40 attributes. \"1\" represents positive while \"-1\" represents negative\n","\n","---"]},{"cell_type":"markdown","metadata":{"_uuid":"cba9cc1ea22168a0da81d39181ef5da1951cb1f4"},"source":["### Import all the required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"49d886506abe4d6d3ec2729036966a0729ee56d8","execution":{"iopub.execute_input":"2023-12-08T17:57:26.228570Z","iopub.status.busy":"2023-12-08T17:57:26.227806Z","iopub.status.idle":"2023-12-08T17:57:27.736312Z","shell.execute_reply":"2023-12-08T17:57:27.733935Z","shell.execute_reply.started":"2023-12-08T17:57:26.228469Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import cv2    \n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import f1_score\n","\n","from keras.applications.inception_v3 import InceptionV3, preprocess_input\n","from keras import optimizers\n","from keras.models import Sequential, Model \n","from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n","from keras.callbacks import ModelCheckpoint\n","from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n","from keras.utils import np_utils\n","from keras.optimizers import SGD\n","\n","from IPython.core.display import display, HTML\n","from PIL import Image\n","from io import BytesIO\n","import base64\n","\n","plt.style.use('ggplot')\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"86450cb83d348d839f1bfa14f2655166fe626407","trusted":true},"outputs":[],"source":["import tensorflow as tf\n","print(tf.__version__)"]},{"cell_type":"markdown","metadata":{"_uuid":"352244740550c3cfb275c496add9c71bc9865b41"},"source":["## Step 1: Data Exploration\n","\n","We will be using the CelebA Dataset, which includes images of 178 x 218 px. Below is an example of how the pictures looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c269fd3d69a5b8ffaad47d56a85c785d6d7be5c1","trusted":true},"outputs":[],"source":["# set variables \n","main_folder = '../input/celeba-dataset/'\n","images_folder = main_folder + 'img_align_celeba/img_align_celeba/'\n","\n","EXAMPLE_PIC = images_folder + '000044.jpg'\n","\n","TRAINING_SAMPLES = 10000\n","VALIDATION_SAMPLES = 2000\n","TEST_SAMPLES = 2000\n","IMG_WIDTH = 178\n","IMG_HEIGHT = 218\n","BATCH_SIZE = 16\n","NUM_EPOCHS = 20"]},{"cell_type":"markdown","metadata":{"_uuid":"d4effe59d9137ff22c55ae1f331772e4e728fb5e"},"source":["### Load the attributes of every picture\n","File: list_attr_celeba.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"1a6e65c380d3050be07f88488f495e34c76fc860","trusted":true},"outputs":[],"source":["# import the data set that include the attribute for each picture\n","df_attr = pd.read_csv(main_folder + 'list_attr_celeba.csv')\n","df_attr.set_index('image_id', inplace=True)\n","df_attr.replace(to_replace=-1, value=0, inplace=True) #replace -1 by 0\n","df_attr.shape"]},{"cell_type":"markdown","metadata":{"_uuid":"3b67c72c9f346f0cdb1901e0b8b79ef37a5468bc"},"source":["### List of the available attribute in the CelebA dataset\n","\n","40 Attributes"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2b79520895583694e69eef65aeadad065ffe4839","trusted":true},"outputs":[],"source":["# List of available attributes\n","for i, j in enumerate(df_attr.columns):\n","    print(i, j)"]},{"cell_type":"markdown","metadata":{"_uuid":"809cda0aead0c34725c4516a5e39aa1a9ad40c6c"},"source":["### Example of a picture in CelebA dataset\n","178 x 218 px"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c0acd2a32edb087315588d3fb9a7cf13477cdbcc","trusted":true},"outputs":[],"source":["# plot picture and attributes\n","img = load_img(EXAMPLE_PIC)\n","plt.grid(False)\n","plt.imshow(img)\n","df_attr.loc[EXAMPLE_PIC.split('/')[-1]][['Smiling','Male','Young']] #some attributes\n","print(\"Example image\")"]},{"cell_type":"markdown","metadata":{"_uuid":"31337d0dbbed4ba6009884b8688860bab8b42c18"},"source":["### Distribution of the Attribute\n","\n","As specified before, this Notebook is an imagine recognition project of the Gender. There are more Female gender than Male gender in the data set. This give us some insight about the need to balance the data in next steps."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2153575008289555fe054216eeb0c946f5d34555","trusted":true},"outputs":[],"source":["# Female or Male\n","plt.title('Female or Male')\n","sns.countplot(y='Male', data=df_attr, color=\"r\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"ba04abfd3a6f51409bdfec909afcfd44c67421aa"},"source":["## Step 2: Split Dataset into Training, Validation and Test\n","\n","The recommended partitioning of images into training, validation, testing of the data set is: \n","* 1-162770 are training\n","* 162771-182637 are validation\n","* 182638-202599 are testing\n","\n","The partition is in file <b>list_eval_partition.csv</b>\n","\n","Due time execution, by now we will be using a reduced number of images:\n","\n","* Training 20000 images\n","* Validation 5000 images\n","* Test 5000 Images\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"60fd112d3dfb79ae7ecf6963d74919f6d44d7c76","trusted":true},"outputs":[],"source":["# Recomended partition\n","df_partition = pd.read_csv(main_folder + 'list_eval_partition.csv')\n","df_partition.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e1328dcf8b461e037de24c18cd528791766c8f57","trusted":true},"outputs":[],"source":["# display counter by partition\n","# 0 -> TRAINING\n","# 1 -> VALIDATION\n","# 2 -> TEST\n","df_partition['partition'].value_counts().sort_index()"]},{"cell_type":"markdown","metadata":{"_uuid":"bbb655b199e34c8dbee371b67eda6c75e26ad1e1"},"source":["#### Join the partition and the attributes in the same data frame"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2e03c64bb8d57dcf39de372fabcf78e5c6c39985","trusted":true},"outputs":[],"source":["# join the partition with the attributes\n","df_partition.set_index('image_id', inplace=True)\n","df_par_attr = df_partition.join(df_attr['Male'], how='inner')\n","df_par_attr.head()"]},{"cell_type":"markdown","metadata":{"_uuid":"6becbeba1d5fefc4416e041cef5f5d701dacc4aa"},"source":["### 2.1: Generate Partitions (Train, Validation, Test)\n","\n","Number of images need to be balanced in order to get a good performance for the model, each model will have its own folder of training, validation and test balanced data.\n","\n","This degree project explains how imbalanced training data impact on CNNs models:\n","\n","https://www.kth.se/social/files/588617ebf2765401cfcc478c/PHensmanDMasko_dkand15.pdf\n","\n","On this step we will create functions that will help us to create each partition."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"56b4af1f2f980957659c0272b92c1f77c13162c4","trusted":true},"outputs":[],"source":["def load_reshape_img(fname):\n","    img = load_img(fname)\n","    x = img_to_array(img)/255.\n","    x = x.reshape((1,) + x.shape)\n","\n","    return x\n","\n","\n","def generate_df(partition, attr, num_samples):\n","    '''\n","    partition\n","        0 -> train\n","        1 -> validation\n","        2 -> test\n","    \n","    '''\n","    \n","    df_ = df_par_attr[(df_par_attr['partition'] == partition) \n","                           & (df_par_attr[attr] == 0)].sample(int(num_samples/2))\n","    df_ = pd.concat([df_,\n","                      df_par_attr[(df_par_attr['partition'] == partition) \n","                                  & (df_par_attr[attr] == 1)].sample(int(num_samples/2))])\n","\n","    # for Train and Validation\n","    if partition != 2:\n","        x_ = np.array([load_reshape_img(images_folder + fname) for fname in df_.index])\n","        x_ = x_.reshape(x_.shape[0], 218, 178, 3)\n","        y_ = np_utils.to_categorical(df_[attr],2)\n","    # for Test\n","    else:\n","        x_ = []\n","        y_ = []\n","\n","        for index, target in df_.iterrows():\n","            im = cv2.imread(images_folder + index)\n","            im = cv2.resize(cv2.cvtColor(im, cv2.COLOR_BGR2RGB), (IMG_WIDTH, IMG_HEIGHT)).astype(np.float32) / 255.0\n","            im = np.expand_dims(im, axis =0)\n","            x_.append(im)\n","            y_.append(target[attr])\n","\n","    return x_, y_"]},{"cell_type":"markdown","metadata":{"_uuid":"659ad807ced4e62a900c31fb5d53ae81be550880"},"source":["## Step 3: Pre-processing Images: Data Augmentation\n","\n","Generates Data Augmentation for iamges.\n","\n","Data Augmentation allows to generate images with modifications to the original ones. The model will learn from these variations (changing angle, size and position), being able to predict better never seen images that could have the same variations in position, size and position."]},{"cell_type":"markdown","metadata":{"_uuid":"19a23bdb3f3f9542aa4743d5ae249ef3b4b57d3b"},"source":["### 3.1. Let's start with an example: Data Augmentation\n","\n","This is how an image will look like after data augmentation (based in the giving parameters below)."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"6a26680b19401c291da42a6d59f894cef40c92f7","trusted":true},"outputs":[],"source":["# Generate image generator for data augmentation\n","datagen =  ImageDataGenerator(\n","  #preprocessing_function=preprocess_input,\n","  rotation_range=30,\n","  width_shift_range=0.2,\n","  height_shift_range=0.2,\n","  shear_range=0.2,\n","  zoom_range=0.2,\n","  horizontal_flip=True\n",")\n","\n","# load one image and reshape\n","img = load_img(EXAMPLE_PIC)\n","x = img_to_array(img)/255.\n","x = x.reshape((1,) + x.shape)\n","\n","# plot 10 augmented images of the loaded iamge\n","plt.figure(figsize=(20,10))\n","plt.suptitle('Data Augmentation', fontsize=28)\n","\n","i = 0\n","for batch in datagen.flow(x, batch_size=1):\n","    plt.subplot(3, 5, i+1)\n","    plt.grid(False)\n","    plt.imshow( batch.reshape(218, 178, 3))\n","    \n","    if i == 9:\n","        break\n","    i += 1\n","    \n","plt.show()"]},{"cell_type":"markdown","metadata":{"_uuid":"6072f6676eb6fee3084da3df56d8e4a3f55a9868"},"source":["The result is a new set of images with modifications from the original one, that allows to the model to learn from these variations in order to take this kind of images during the learning process and predict better never seen images."]},{"cell_type":"markdown","metadata":{"_uuid":"768e425237481c6bb6b980272fe63137bbbc20df"},"source":["### 3.2. Build Data Generators"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"35bbdcb65be9f69fc0ed02e28c946d4419a4335b","trusted":true},"outputs":[],"source":["# Train data\n","x_train, y_train = generate_df(0, 'Male', TRAINING_SAMPLES)\n","\n","train_datagen =  ImageDataGenerator(\n","  preprocessing_function=preprocess_input,\n","  rotation_range=30,\n","  width_shift_range=0.2,\n","  height_shift_range=0.2,\n","  shear_range=0.2,\n","  zoom_range=0.2,\n","  horizontal_flip=True,\n",")\n","\n","train_datagen.fit(x_train)\n","\n","train_generator = train_datagen.flow(\n","    x_train, \n","    y_train,\n","    batch_size=BATCH_SIZE,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"2dbaf8b9a50fc5e524436fb3c2202f6e0053c86a","trusted":true},"outputs":[],"source":["# Validation Data\n","x_valid, y_valid = generate_df(1, 'Male', VALIDATION_SAMPLES)"]},{"cell_type":"markdown","metadata":{"_uuid":"5496743ce168a108a84e7f21b60f3f58309bbaa0"},"source":["With the data generator created and data for validation, we are ready to start modeling."]},{"cell_type":"markdown","metadata":{"_uuid":"c729337fbac35a302c96a49faaf122dac082c8c2"},"source":["## Step 4: Build the Model - Gender Recognition"]},{"cell_type":"markdown","metadata":{"_uuid":"8cd7789c59348ff27b6556bda4338d51715e4c87"},"source":["### 4.1. Set the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"_uuid":"7e0dbc3b3d11876f7a3dc36e1f67ac093978f526","trusted":true},"outputs":[],"source":["# Import InceptionV3 Model\n","inc_model = InceptionV3(weights='../input/inceptionv3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5',\n","                        include_top=False,\n","                        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n","\n","print(\"number of layers:\", len(inc_model.layers))"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["inc_model.summary()"]},{"cell_type":"markdown","metadata":{"_uuid":"ad941871e156206d4dee61879e357e808748fece"},"source":["<h2>Inception-V3 model structure</h2>"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"232d3dc90e0e8859c596e6113b5dca8571c84d78","trusted":true},"outputs":[],"source":["#Adding custom Layers\n","x = inc_model.output\n","x = GlobalAveragePooling2D()(x)\n","x = Dense(1024, activation=\"relu\")(x)\n","x = Dropout(0.5)(x)\n","x = Dense(512, activation=\"relu\")(x)\n","predictions = Dense(2, activation=\"softmax\")(x)"]},{"cell_type":"markdown","metadata":{"_uuid":"5644135b7f94522ff2699373d7efe539b36e07f9"},"source":["<h2>New Top layers</h2>\n","Layers to be trained with the new model."]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"3d24a13d3e0944d5dcf008fe6f9c85ec4e702a01","trusted":true},"outputs":[],"source":["# creating the final model \n","model_ = Model(inputs=inc_model.input, outputs=predictions)\n","\n","# Lock initial layers to do not be trained\n","for layer in model_.layers[:52]:\n","    layer.trainable = False\n","\n","# compile the model\n","model_.compile(optimizer=SGD(lr=0.0001, momentum=0.9)\n","                    , loss='categorical_crossentropy'\n","                    , metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"_uuid":"0c84c56204fc441e54ed51b47e967a02cf4a5041"},"source":["### 4.2. Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b16d04820bd6ff6dd7bc24fbe51a7cc12ae47eec","trusted":true},"outputs":[],"source":["checkpointer = ModelCheckpoint(filepath='weights.best.inc.male.hdf5', \n","                               verbose=1, save_best_only=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"a8808fbd97d7158400cbe1677e78ff30bb2b12a9","scrolled":true,"trusted":true},"outputs":[],"source":["hist = model_.fit_generator(train_generator\n","                     , validation_data = (x_valid, y_valid)\n","                      , steps_per_epoch= TRAINING_SAMPLES/BATCH_SIZE\n","                      , epochs= NUM_EPOCHS\n","                      , callbacks=[checkpointer]\n","                      , verbose=1\n","                    )"]},{"cell_type":"markdown","metadata":{"_uuid":"72e75a244fdf073d8f1278fa525de6c8ccf9a41e"},"source":["#### The best model after NUM_epech got an accuracy over the validation data of 95.75%."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def visualize_training(history, lw = 3):\n","    plt.figure(figsize=(10,10))\n","    plt.subplot(2,1,1)\n","    plt.plot(history.history['acc'], label = 'training', marker = '*', linewidth = lw)\n","    plt.plot(history.history['val_acc'], label = 'validation', marker = 'o', linewidth = lw)\n","    plt.title('Accuracy Comparison')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.grid(True)\n","    plt.legend(fontsize = 'x-large')\n","\n","    plt.subplot(2,1,2)\n","    plt.plot(history.history['loss'], label = 'training', marker = '*', linewidth = lw)\n","    plt.plot(history.history['val_loss'], label = 'validation', marker = 'o', linewidth = lw)\n","    plt.title('Loss Comparison')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend(fontsize = 'x-large')\n","    plt.grid(True)\n","    plt.show()\n","\n","\n","visualize_training(hist)"]},{"cell_type":"markdown","metadata":{"_uuid":"56f81f1620f273e93e2d194752d6b309308bf29e"},"source":["### 4.3. Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"61f890047aa886fa8942867ad76d3a73104b1360","trusted":true},"outputs":[],"source":["#load the best model\n","model_.load_weights('weights.best.inc.male.hdf5')"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"e77ebcbefdd61f27f0cfe02a13f30134ea3921d5","trusted":true},"outputs":[],"source":["# Test Data\n","x_test, y_test = generate_df(2, 'Male', TEST_SAMPLES)\n","\n","# generate prediction\n","model_predictions = [np.argmax(model_.predict(feature)) for feature in x_test ]\n","\n","# report test accuracy\n","test_accuracy = 100 * np.sum(np.array(model_predictions)==y_test) / len(model_predictions)\n","print('Model Evaluation')\n","print('Test accuracy: %.4f%%' % test_accuracy)\n","print('f1_score:', f1_score(y_test, model_predictions))"]},{"cell_type":"markdown","metadata":{"_uuid":"9efc0a6de2ff07268a946d897b6615c26e795e97"},"source":["The built model using transfer learning from the InceptionV3 and adding custom layers successfully recognize the gender giving certain picture with <b>92.6% of accuracy over the test data</b>."]},{"cell_type":"markdown","metadata":{"_uuid":"611ac23970abdadaae912cc3e9e3779265bd9fc1"},"source":["### 6. Let's play with the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5ad0cdabb6e82d73de3815a10cebf9f8df4b378b","trusted":true},"outputs":[],"source":["gender_target = {0: 'Female'\n","                , 1: 'Male'}\n","\n","def img_to_display(filename):\n","    \n","    i = Image.open(filename)\n","    i.thumbnail((200, 200), Image.LANCZOS)\n","    \n","    with BytesIO() as buffer:\n","        i.save(buffer, 'jpeg')\n","        return base64.b64encode(buffer.getvalue()).decode()\n","    \n","\n","def display_result(filename, prediction, target):\n","    gender = 'Male'\n","    gender_icon = \"https://png.pngtree.com/png-clipart/20190705/original/pngtree-man-avatar-icon-professional-man-character-png-image_4356027.jpg\"\n","        \n","    if prediction[1] <= 0.5:\n","        gender_icon = \"https://png.pngtree.com/png-clipart/20190614/original/pngtree-female-avatar-vector-icon-png-image_3725439.jpg\"\n","        gender = 'Female'\n","            \n","    display_html = '''\n","    <div style=\"overflow: auto;  border: 2px solid #D8D8D8;\n","        padding: 5px; width: 480px;\" >\n","        <img src=\"data:image/jpeg;base64,{}\" style=\"float: left;\" width=\"200\" height=\"200\">\n","        <div style=\"padding: 10px 0px 0px 20px; overflow: auto;\">\n","            <img src=\"{}\" style=\"float: left;\" width=\"40\" height=\"40\">\n","            <h3 style=\"margin-left: 50px; margin-top: 2px;\">Prediction: {}</h3>\n","            <p style=\"margin-left: 50px; margin-top: -6px; font-size: 12px\">{} probability</p>\n","            <p style=\"margin-left: 50px; margin-top: -16px; font-size: 12px\">Real gender: {}</p>\n","        </div>\n","    </div>\n","    '''.format(img_to_display(filename)\n","               , gender_icon\n","               , gender\n","               , \"{0:.2f}%\".format(round(max(prediction)*100,2))\n","               , gender_target[target]\n","               , filename.split('/')[-1]\n","               )\n","\n","    display(HTML(display_html))"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"19c28df51f9d3421ca2a89452ccc3f89509ff127","trusted":true},"outputs":[],"source":["def gender_prediction(filename):\n","    '''\n","    predict the gender\n","    \n","    input: filename: str of the file name\n","        \n","    return: array of the prob of the targets.\n","    \n","    '''\n","    im = cv2.imread(filename)\n","    im = cv2.resize(cv2.cvtColor(im, cv2.COLOR_BGR2RGB), (178, 218)).astype(np.float32) / 255.0\n","    im = np.expand_dims(im, axis =0)\n","    \n","    # prediction\n","    result = model_.predict(im)\n","    prediction = np.argmax(result)\n","    \n","    return result"]},{"cell_type":"markdown","metadata":{},"source":["### Prediction on 10 random test images"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"c272e8b54cccf4138f6ff806292ab4e5cda97db9","trusted":true},"outputs":[],"source":["#select random images of the test partition\n","df_to_test = df_par_attr[(df_par_attr['partition'] == 2)].sample(10)\n","\n","for index, target in df_to_test.iterrows():\n","    result = gender_prediction(images_folder + index)\n","    \n","    #display result\n","    display_result(images_folder + index, result[0], target['Male'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":6306,"sourceId":9901,"sourceType":"datasetVersion"},{"datasetId":29561,"sourceId":37705,"sourceType":"datasetVersion"}],"dockerImageVersionId":11105,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":4}
